{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea8dfae-5f2b-4560-9ac5-fec18d2a381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans, LDA\n",
    "from pyspark.ml.feature import VectorAssembler, RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import joblib\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pyspark.sql.functions as f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e317164a-cf78-4786-b1f9-677bd3281e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealTimeModelTraining\") \\\n",
    "    .config('spark.jars.packages','org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4')\\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16f26ba-475e-4d1a-9a7b-003180386eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Kafka\n",
    "\n",
    "streaming = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"filtered_stream\") \\\n",
    "    .option(\"startingOffsets\", \"latest\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c777a23b-cfca-432b-b481-74b51f9cc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the value (message) from the Kafka stream\n",
    "messages = streaming.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eced24f7-4d2b-4801-8bee-6983f66c5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON message\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d2dce4-916a-47fc-99e7-5ab62ce98275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON\n",
    "messages_parsed = messages.select(from_json(messages[\"value\"], schema).alias(\"data\"))\n",
    "df = messages_parsed.select(\"data.text\", \"data.timestamp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1590684f-86f3-4849-bbf8-28d58ae9b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data (your pipeline steps here)\n",
    "df_cleaned = df.withColumn(\"datetime_clean\", f.regexp_replace(\"timestamp\", r\"^[A-Za-z]{3} \", \"\")) \\\n",
    "               .withColumn(\"datetime_column2\", f.regexp_replace(\"datetime_clean\", r\"\\s[A-Za-z]+$\", \"\")) \\\n",
    "               .withColumn(\"datetime_final\", f.to_timestamp(\"datetime_column2\", \"MMM dd HH:mm:ss z yyyy\")) \\\n",
    "               .drop(\"datetime_clean\", \"datetime_column2\")  # Drop unnecessary columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6e0896-6ad4-479d-93ee-c9683bf71cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "from pyspark.ml.clustering import LDA, LDAModel, KMeansModel\n",
    "from pyspark.ml.clustering import LocalLDAModel\n",
    "\n",
    "# Load models\n",
    "cv_model = CountVectorizerModel.load(\"bluesky_cv_model\")\n",
    "lda_model = LocalLDAModel.load(\"bluesky_lda_model\")\n",
    "kmeans_model = KMeansModel.load(\"bluesky_kmeans_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd8c475a-d65b-4b11-be2e-d0cba8719919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process text (RegexTokenizer, StopWordsRemover, etc.)\n",
    "# regex_token = RegexTokenizer(inputCol=\"text\", outputCol=\"content_filtered\", pattern=\"\\\\W+\", toLowercase=True)\n",
    "\n",
    "# default_stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "# additional_stopwords = ['rt', 'via', 'amp', 'https', 'http', 'co', 'bsky', 'u', 'app', 's', 'www', 'com', 'de']\n",
    "# all_stopwords = list(set(default_stopwords + additional_stopwords))\n",
    "\n",
    "# stop_words_remover = StopWordsRemover(inputCol=\"content_filtered\", outputCol=\"filtered_words\", stopWords=all_stopwords)\n",
    "\n",
    "# count_vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "\n",
    "# vector_assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"final_features\")\n",
    "\n",
    "# # Build the pipeline\n",
    "# pipeline = Pipeline(stages=[regex_token, stop_words_remover, count_vectorizer, vector_assembler])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d00b849-987c-4e65-b3f4-3c875f3a022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing Text (no fitting here)\n",
    "regex_token = RegexTokenizer(inputCol=\"text\", outputCol=\"content_filtered\", pattern=\"\\\\W+\", toLowercase=True)\n",
    "df_tokenized = regex_token.transform(df_cleaned)\n",
    "\n",
    "default_stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "additional_stopwords = ['rt', 'via', 'amp', 'https', 'http', 'co', 'bsky', 'u', 'app', 's', 'www', 'com', 'de']\n",
    "all_stopwords = list(set(default_stopwords + additional_stopwords))\n",
    "\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"content_filtered\", outputCol=\"filtered_words\", stopWords=all_stopwords)\n",
    "df_filtered = stop_words_remover.transform(df_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5611d5c-75f8-47f2-9ecb-6975ead081a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After tokenizing and removing stopwords\n",
    "df_vectorized = cv_model.transform(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5430d8f6-0ba3-463e-96d9-ef2bf1c81d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- For LDA ---\n",
    "lda_output = lda_model.transform(df_vectorized) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df27be0-d798-4bbd-aa25-bb3552c5ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- For KMeans ---\n",
    "# Assembling 'features' into 'final_features' first (because KMeans was trained on final_features)\n",
    "vector_assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"final_features\")\n",
    "df_final = vector_assembler.transform(df_vectorized)\n",
    "\n",
    "kmeans_output = kmeans_model.transform(df_final) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db3ff8f-9070-45b7-bd87-3a98bf6edbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Output what you want ---\n",
    "output = kmeans_output.select(\"text\", \"timestamp\", \"cluster\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf94e7b6-b230-48dc-aa23-dcb42106c5e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 03025bea-e486-4983-9f84-7047d0c2a26b, runId = 5c1ec37e-e769-4f03-a651-1804c363f7c7] terminated with exception: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (recovery-vm.us-east4-a.c.strange-theme-447800-g8.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`KMeansModel$$Lambda/0x00007fc55d15a960`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => int).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Both vectors should have same length, found v1 is 4286 while v2 is 4285\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.mllib.util.MLUtils$.fastSquaredDistance(MLUtils.scala:541)\n\tat org.apache.spark.mllib.clustering.EuclideanDistanceMeasure$.fastSquaredDistance(DistanceMeasure.scala:414)\n\tat org.apache.spark.mllib.clustering.EuclideanDistanceMeasure.findClosest(DistanceMeasure.scala:309)\n\tat org.apache.spark.mllib.clustering.DistanceMeasure.findClosest(DistanceMeasure.scala:132)\n\tat org.apache.spark.mllib.clustering.KMeansModel.predict(KMeansModel.scala:91)\n\tat org.apache.spark.ml.clustering.KMeansModel.predict(KMeans.scala:177)\n\tat org.apache.spark.ml.clustering.KMeansModel.$anonfun$transform$1(KMeans.scala:159)\n\tat org.apache.spark.ml.clustering.KMeansModel.$anonfun$transform$1$adapted(KMeans.scala:159)\n\t... 21 more\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-41983b317559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 03025bea-e486-4983-9f84-7047d0c2a26b, runId = 5c1ec37e-e769-4f03-a651-1804c363f7c7] terminated with exception: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (recovery-vm.us-east4-a.c.strange-theme-447800-g8.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`KMeansModel$$Lambda/0x00007fc55d15a960`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => int).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Both vectors should have same length, found v1 is 4286 while v2 is 4285\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.mllib.util.MLUtils$.fastSquaredDistance(MLUtils.scala:541)\n\tat org.apache.spark.mllib.clustering.EuclideanDistanceMeasure$.fastSquaredDistance(DistanceMeasure.scala:414)\n\tat org.apache.spark.mllib.clustering.EuclideanDistanceMeasure.findClosest(DistanceMeasure.scala:309)\n\tat org.apache.spark.mllib.clustering.DistanceMeasure.findClosest(DistanceMeasure.scala:132)\n\tat org.apache.spark.mllib.clustering.KMeansModel.predict(KMeansModel.scala:91)\n\tat org.apache.spark.ml.clustering.KMeansModel.predict(KMeans.scala:177)\n\tat org.apache.spark.ml.clustering.KMeansModel.$anonfun$transform$1(KMeans.scala:159)\n\tat org.apache.spark.ml.clustering.KMeansModel.$anonfun$transform$1$adapted(KMeans.scala:159)\n\t... 21 more\n\nDriver stacktrace:"
     ]
    }
   ],
   "source": [
    "query = output.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5941029-5dfe-4743-8c12-1ad6275c41bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
